#!/usr/bin/env python3
"""
ğŸš€ LLM Platform Runner
This file guarantees the platform will start
"""
import os
import sys
import subprocess
import time

def check_ollama():
    """Check if Ollama is running"""
    try:
        import httpx
        response = httpx.get("http://localhost:11434/api/tags", timeout=2)
        return response.status_code == 200
    except:
        return False

def start_ollama():
    """Start Ollama if not running"""
    print("ğŸ” Checking Ollama...")
    if not check_ollama():
        print("ğŸš€ Starting Ollama...")
        # Try to start Ollama
        try:
            import subprocess
            subprocess.Popen(["ollama", "serve"], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
            print("â³ Waiting for Ollama to start...")
            time.sleep(5)
            
            # Check again
            if check_ollama():
                print("âœ… Ollama started successfully")
            else:
                print("âš  Ollama might take longer to start. Continuing...")
        except Exception as e:
            print(f"âš  Could not start Ollama: {e}")
            print("âš  The platform will work in fallback mode")
    else:
        print("âœ… Ollama is already running")

def pull_default_model():
    """Pull default model if not available"""
    print("ğŸ“¦ Checking for models...")
    try:
        import httpx
        response = httpx.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [m["name"] for m in models]
            
            if "deepseek-coder:6.7b" not in model_names:
                print("ğŸ“¥ Pulling deepseek-coder:6.7b...")
                subprocess.run(["ollama", "pull", "deepseek-coder:6.7b"], 
                             capture_output=True)
                print("âœ… Model pulled")
            else:
                print("âœ… Default model available")
    except Exception as e:
        print(f"âš  Could not check models: {e}")

def start_server():
    """Start the FastAPI server"""
    print("\n" + "="*60)
    print("ğŸš€ STARTING LLM PLATFORM SERVER")
    print("="*60)
    
    # Set environment variables to be safe
    os.environ["DATABASE_URL"] = "sqlite:///./llm_platform.db"
    os.environ["OLLAMA_INSTANCES"] = "http://localhost:11434"
    os.environ["CORS_ORIGINS"] = "http://localhost:3000"
    os.environ["API_KEY"] = "default-api-key"
    
    # Start the server
    import uvicorn
    
    print("\nğŸ“Š Server Information:")
    print(f"   ğŸŒ URL: http://localhost:8000")
    print(f"   ğŸ“š Docs: http://localhost:8000/docs")
    print(f"   ğŸ¥ Health: http://localhost:8000/health")
    print(f"   ğŸ”‘ API Key: default-api-key")
    
    print("\nğŸ“‹ Quick Test Commands:")
    print('   curl http://localhost:8000/health')
    print('   curl -H "Authorization: Bearer default-api-key" http://localhost:8000/api/models')
    print('   curl -X POST http://localhost:8000/api/chat \\')
    print('     -H "Authorization: Bearer default-api-key" \\')
    print('     -H "Content-Type: application/json" \\')
    print('     -d \'{"message": "Write Python hello world"}\'')
    
    print("\n" + "="*60)
    print("ğŸ”„ Starting server... (Press Ctrl+C to stop)")
    print("="*60 + "\n")
    
    # Start uvicorn
    uvicorn.run(
        "src.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
        workers=1
    )

def main():
    """Main function"""
    print("ğŸ”§ LLM Platform Setup")
    print("="*40)
    
    # Check dependencies
    try:
        import fastapi
        import uvicorn
        import httpx
        print("âœ… All dependencies installed")
    except ImportError as e:
        print(f"âŒ Missing dependency: {e}")
        print("ğŸ“¦ Installing dependencies...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", 
                              "fastapi", "uvicorn", "httpx", "python-jose", "passlib"])
    
    # Start Ollama
    start_ollama()
    
    # Pull model
    pull_default_model()
    
    # Start server
    start_server()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Server stopped by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")